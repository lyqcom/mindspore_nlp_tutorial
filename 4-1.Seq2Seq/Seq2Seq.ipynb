{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "metropolitan-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor, ms_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "internal-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data, num_dic, n_step):\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target) # not one-hot\n",
    "\n",
    "    # make tensor\n",
    "    return Tensor(input_batch, mindspore.float32), Tensor(output_batch, mindspore.float32), Tensor(target_batch, mindspore.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compressed-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Seq2Seq(nn.Cell):\n",
    "    def __init__(self, n_class, n_hidden, dropout):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=dropout)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=dropout)\n",
    "        self.fc = nn.Dense(n_hidden, n_class)\n",
    "        \n",
    "        \n",
    "    def construct(self, enc_input, dec_input):\n",
    "        enc_input = enc_input.transpose((1, 0, 2)) # enc_input: [max_len(=n_step, time step), batch_size, n_class]\n",
    "        dec_input = dec_input.transpose((1, 0, 2)) # dec_input: [max_len(=n_step, time step), batch_size, n_class]\n",
    "\n",
    "        # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input)\n",
    "        # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "impaired-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 5\n",
    "n_hidden = 128\n",
    "dropout = 0.5\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "n_class = len(num_dic)\n",
    "batch_size = len(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "structured-external",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(257088:139675582087424,MainProcess):2022-08-12-21:11:28.654.667 [mindspore/nn/layer/rnns.py:392] dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "[WARNING] ME(257088:139675582087424,MainProcess):2022-08-12-21:11:28.663.657 [mindspore/nn/layer/rnns.py:392] dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(n_class, n_hidden, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "japanese-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "governmental-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, output_batch, target_batch = make_batch(seq_data, num_dic, n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20affee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(enc_input, dec_input, target):\n",
    "    output = model(enc_input, dec_input)\n",
    "    output = output.transpose((1, 0, 2))\n",
    "    return criterion(output.view(-1, output.shape[-1]), target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0727166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = ops.value_and_grad(forward, None, optimizer.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd24c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ms_function\n",
    "def train_step(enc_input, dec_input, target):\n",
    "    loss, grads = grad_fn(enc_input, dec_input, target)\n",
    "    optimizer(grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "celtic-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.000974\n",
      "Epoch: 2000 cost = 0.000262\n",
      "Epoch: 3000 cost = 0.000112\n",
      "Epoch: 4000 cost = 0.000056\n",
      "Epoch: 5000 cost = 0.000030\n"
     ]
    }
   ],
   "source": [
    "model.set_train()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # input_batch : [batch_size, max_len(=n_step, time step), n_class]\n",
    "    # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\n",
    "    # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\n",
    "    loss = train_step(input_batch, output_batch, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss.asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "resident-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "man -> women\n",
      "mans -> women\n",
      "king -> queen\n",
      "black -> white\n",
      "upp -> down\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "# Test\n",
    "def translate(word):\n",
    "    input_batch, output_batch, _ = make_batch([[word, 'P' * len(word)]],  num_dic, n_step)\n",
    "    output = model(input_batch, output_batch)\n",
    "    # output : [max_len+1(=6), batch_size(=1), n_class]\n",
    "\n",
    "    predict = output.asnumpy().argmax(2) # select n_class dimension\n",
    "    decoded = [char_arr[i[0]] for i in predict]\n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "\n",
    "    return translated.replace('P', '')\n",
    "\n",
    "print('test')\n",
    "print('man ->', translate('man'))\n",
    "print('mans ->', translate('mans'))\n",
    "print('king ->', translate('king'))\n",
    "print('black ->', translate('black'))\n",
    "print('upp ->', translate('upp'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ms1.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd0943702584cdb580f8947884f31a9fb49482f77f8c89ed6532de3aa180e7ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
